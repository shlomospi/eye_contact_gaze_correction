{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load package and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import dlib\n",
    "import time\n",
    "import socket\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from win32api import GetSystemMetrics\n",
    "import win32gui\n",
    "\n",
    "from threading import Thread, Lock\n",
    "import multiprocessing as mp\n",
    "from config import get_config\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf,_ = get_config()\n",
    "if conf.mod == 'flx':\n",
    "    import flx as model\n",
    "else:\n",
    "    sys.exit(\"Wrong Model selection: flx or deepwarp\")\n",
    "\n",
    "# system parameters\n",
    "model_dir = './'+conf.weight_set+'/warping_model/'+conf.mod+'/'+ str(conf.ef_dim) + '/'\n",
    "size_video = [640,480]\n",
    "# fps = 0\n",
    "P_IDP = 5\n",
    "depth = -50\n",
    "# for monitoring\n",
    "\n",
    "# environment parameter\n",
    "Rs = (GetSystemMetrics(0),GetSystemMetrics(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir\n",
    "print(Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video receiver\n",
    "class video_receiver:\n",
    "    def __init__(self,shared_v,lock):\n",
    "        self.close = False\n",
    "        self.video_recv = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n",
    "        print('Socket created')\n",
    "        #         global remote_head_Center\n",
    "        self.video_recv.bind(('',conf.recver_port))\n",
    "        self.video_recv.listen(10)\n",
    "        print('Socket now listening')\n",
    "        self.conn, self.addr=self.video_recv.accept()\n",
    "        # face detection\n",
    "        self.detector = dlib.get_frontal_face_detector()\n",
    "        self.predictor = dlib.shape_predictor(\"./lm_feat/shape_predictor_68_face_landmarks.dat\") \n",
    "        self.face_detect_size = [320,240]\n",
    "        self.x_ratio = size_video[0]/self.face_detect_size[0]\n",
    "        self.y_ratio = size_video[1]/self.face_detect_size[1]      \n",
    "        self.start_recv(shared_v,lock)\n",
    "\n",
    "    def face_detection(self,frame,shared_v,lock):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        face_detect_gray = cv2.resize(gray,(self.face_detect_size[0],self.face_detect_size[1]))\n",
    "        detections = self.detector(face_detect_gray, 0)\n",
    "        coor_remote_head_center=[0,0]\n",
    "        for k,bx in enumerate(detections):\n",
    "            coor_remote_head_center = [int((bx.left()+bx.right())*self.x_ratio/2),\n",
    "                                       int((bx.top()+bx.bottom())*self.y_ratio/2)]\n",
    "            break\n",
    "        # share remote participant's eye to the main process\n",
    "        lock.acquire()\n",
    "        shared_v[0] = coor_remote_head_center[0]\n",
    "        shared_v[1] = coor_remote_head_center[1]\n",
    "        lock.release()\n",
    "\n",
    "    def start_recv(self,shared_v,lock):\n",
    "        data = b\"\"\n",
    "        payload_size = struct.calcsize(\"L\")\n",
    "        print(\"payload_size: {}\".format(payload_size))\n",
    "        while True:\n",
    "            while len(data) < payload_size:\n",
    "                data += self.conn.recv(4096)\n",
    "\n",
    "            packed_msg_size = data[:payload_size]\n",
    "            data = data[payload_size:]\n",
    "            msg_size = struct.unpack(\"L\", packed_msg_size)[0]\n",
    "            while len(data) < msg_size:\n",
    "                data += self.conn.recv(4096)\n",
    "\n",
    "            frame_data = data[:msg_size]\n",
    "            data = data[msg_size:]\n",
    "            frame = pickle.loads(frame_data, fix_imports=True, encoding=\"bytes\")\n",
    "            if frame == 'stop':\n",
    "                print('stop')\n",
    "                cv2.destroyWindow(\"Remote\")\n",
    "                break\n",
    "            \n",
    "            frame = cv2.imdecode(frame, cv2.IMREAD_COLOR)\n",
    "            \n",
    "            # face detection\n",
    "            self.video_recv_hd_thread = Thread(target=self.face_detection, args=(frame,shared_v,lock))\n",
    "            self.video_recv_hd_thread.start()\n",
    "        \n",
    "            cv2.imshow('Remote',frame)\n",
    "            cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flx-gaze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gaze_redirection_system:\n",
    "    def __init__(self,shared_v,lock):\n",
    "        #Landmark identifier. Set the filename to whatever you named the downloaded file\n",
    "        self.detector = dlib.get_frontal_face_detector()\n",
    "        self.predictor = dlib.shape_predictor(\"./lm_feat/shape_predictor_68_face_landmarks.dat\") \n",
    "        self.size_df = (320,240)\n",
    "        self.size_I = (48,64)\n",
    "        # initial value\n",
    "        self.Rw = [0,0]\n",
    "        self.Pe_z = -60\n",
    "        #### get configurations\n",
    "        self.f = conf.f\n",
    "        self.Ps = (conf.S_W,conf.S_H)\n",
    "        self.Pc = (conf.P_c_x,conf.P_c_y,conf.P_c_z)\n",
    "        self.Pe = [self.Pc[0],self.Pc[1],self.Pe_z] # H,V,D\n",
    "        ## start video sender\n",
    "        self.client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.client_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "        self.client_socket.connect((conf.tar_ip, conf.sender_port))\n",
    "        self.encode_param=[int(cv2.IMWRITE_JPEG_QUALITY),90]\n",
    "        \n",
    "        # load model to gpu\n",
    "        print(\"Loading model of [L] eye to GPU\")\n",
    "        with tf.Graph().as_default() as g:\n",
    "            # define placeholder for inputs to network\n",
    "            with tf.name_scope('inputs'):\n",
    "                self.LE_input_img = tf.placeholder(tf.float32, [None, conf.height, conf.width, conf.channel], name=\"input_img\")\n",
    "                self.LE_input_fp = tf.placeholder(tf.float32, [None, conf.height, conf.width,conf.ef_dim], name=\"input_fp\")\n",
    "                self.LE_input_ang = tf.placeholder(tf.float32, [None, conf.agl_dim], name=\"input_ang\")\n",
    "                self.LE_phase_train = tf.placeholder(tf.bool, name='phase_train') # a bool for batch_normalization\n",
    "\n",
    "            self.LE_img_pred, _, _ = model.inference(self.LE_input_img, self.LE_input_fp, self.LE_input_ang, self.LE_phase_train, conf)\n",
    "\n",
    "            # split modle here\n",
    "            self.L_sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False), graph = g)\n",
    "            # load model\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            ckpt = tf.train.get_checkpoint_state(model_dir+'L/')\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                # Restores from checkpoint\n",
    "                saver.restore(self.L_sess, ckpt.model_checkpoint_path)\n",
    "            else:\n",
    "                print('No checkpoint file found')\n",
    "\n",
    "        print(\"Loading model of [R] eye to GPU\")\n",
    "        with tf.Graph().as_default() as g2:\n",
    "            # define placeholder for inputs to network\n",
    "            with tf.name_scope('inputs'):\n",
    "                self.RE_input_img = tf.placeholder(tf.float32, [None, conf.height, conf.width, conf.channel], name=\"input_img\")\n",
    "                self.RE_input_fp = tf.placeholder(tf.float32, [None, conf.height, conf.width,conf.ef_dim], name=\"input_fp\")\n",
    "                self.RE_input_ang = tf.placeholder(tf.float32, [None, conf.agl_dim], name=\"input_ang\")\n",
    "                self.RE_phase_train = tf.placeholder(tf.bool, name='phase_train') # a bool for batch_normalization\n",
    "\n",
    "            self.RE_img_pred, _, _ = model.inference(self.RE_input_img, self.RE_input_fp, self.RE_input_ang, self.RE_phase_train, conf)\n",
    "\n",
    "            # split modle here\n",
    "            self.R_sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False), graph = g2)\n",
    "            # load model\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            ckpt = tf.train.get_checkpoint_state(model_dir+'R/')\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                # Restores from checkpoint\n",
    "                saver.restore(self.R_sess, ckpt.model_checkpoint_path)\n",
    "            else:\n",
    "                print('No checkpoint file found')\n",
    "               \n",
    "        self.run(shared_v,lock)\n",
    "        \n",
    "    def monitor_para(self,frame,fig_alpha,fig_eye_pos,fig_R_w):\n",
    "        cv2.rectangle(frame,\n",
    "                  (size_video[0]-150,0),(size_video[0],55),\n",
    "                  (255,255,255),-1\n",
    "                 )\n",
    "        cv2.putText(frame,\n",
    "                    'Eye:['+str(int(fig_eye_pos[0])) +','+str(int(fig_eye_pos[1]))+','+str(int(fig_eye_pos[2]))+']',\n",
    "                    (size_video[0]-140,15), cv2.FONT_HERSHEY_SIMPLEX, 0.4,(0,0,255),1,cv2.LINE_AA)\n",
    "        cv2.putText(frame,\n",
    "                    'alpha:[V='+str(int(fig_alpha[0])) + ',H='+ str(int(fig_alpha[1]))+']',\n",
    "                    (size_video[0]-140,30),cv2.FONT_HERSHEY_SIMPLEX,0.4,(0,0,255),1,cv2.LINE_AA)\n",
    "        cv2.putText(frame,\n",
    "                    'R_w:['+str(int(fig_R_w[0])) + ','+ str(int(fig_R_w[1]))+']',\n",
    "                    (size_video[0]-140,45),cv2.FONT_HERSHEY_SIMPLEX,0.4,(0,0,255),1,cv2.LINE_AA)\n",
    "        return frame\n",
    "        \n",
    "    def get_inputs(self, frame, shape, pos = \"L\", size_I = [48,64]):\n",
    "        if(pos == \"R\"):\n",
    "            lc = 36\n",
    "            rc = 39\n",
    "            FP_seq = [36,37,38,39,40,41]\n",
    "        elif(pos == \"L\"):\n",
    "            lc = 42\n",
    "            rc = 45\n",
    "            FP_seq = [45,44,43,42,47,46]\n",
    "        else:\n",
    "            print(\"Error: Wrong Eye\")\n",
    "\n",
    "        eye_cx = (shape.part(rc).x+shape.part(lc).x)*0.5\n",
    "        eye_cy = (shape.part(rc).y+shape.part(lc).y)*0.5\n",
    "        eye_center = [eye_cx, eye_cy]\n",
    "        eye_len = np.absolute(shape.part(rc).x - shape.part(lc).x)\n",
    "        bx_d5w = eye_len*3/4\n",
    "        bx_h = 1.5*bx_d5w\n",
    "        sft_up = bx_h*7/12\n",
    "        sft_low = bx_h*5/12\n",
    "        img_eye = frame[int(eye_cy-sft_up):int(eye_cy+sft_low),int(eye_cx-bx_d5w):int(eye_cx+bx_d5w)]\n",
    "        ori_size = [img_eye.shape[0],img_eye.shape[1]]\n",
    "        LT_coor = [int(eye_cy-sft_up), int(eye_cx-bx_d5w)] # (y,x)    \n",
    "        img_eye = cv2.resize(img_eye, (size_I[1],size_I[0]))\n",
    "        # create anchor maps\n",
    "        ach_map = []\n",
    "        for i,d in enumerate(FP_seq):\n",
    "            resize_x = int((shape.part(d).x-LT_coor[1])*size_I[1]/ori_size[1])\n",
    "            resize_y = int((shape.part(d).y-LT_coor[0])*size_I[0]/ori_size[0])\n",
    "            # y\n",
    "            ach_map_y = np.expand_dims(np.expand_dims(np.arange(0, size_I[0]) - resize_y, axis=1), axis=2)\n",
    "            ach_map_y = np.tile(ach_map_y, [1,size_I[1],1])\n",
    "            # x\n",
    "            ach_map_x = np.expand_dims(np.expand_dims(np.arange(0, size_I[1]) - resize_x, axis=0), axis=2)\n",
    "            ach_map_x = np.tile(ach_map_x, [size_I[0],1,1])\n",
    "            if (i ==0):\n",
    "                ach_map = np.concatenate((ach_map_x, ach_map_y), axis=2)\n",
    "            else:\n",
    "                ach_map = np.concatenate((ach_map, ach_map_x, ach_map_y), axis=2)\n",
    "\n",
    "        return img_eye/255, ach_map, eye_center, ori_size, LT_coor\n",
    "       \n",
    "    def shifting_angles_estimator(self, R_le, R_re,shared_v,lock):\n",
    "        # get P_w\n",
    "        try:\n",
    "            tar_win = win32gui.FindWindow(None, \"Remote\")\n",
    "            #left, top, reight, bottom\n",
    "            Rw_lt = win32gui.GetWindowRect(tar_win)\n",
    "            size_window = (Rw_lt[2]-Rw_lt[0], Rw_lt[3]-Rw_lt[1])\n",
    "        except:\n",
    "            Rw_lt = [int(Rs[0])-int(size_window[0]/2),int(Rs[1])-int(size_window[1]/2)]\n",
    "            size_window = (659,528)\n",
    "            print(\"Missing the window\")\n",
    "        # get pos head\n",
    "        pos_remote_head = [int(size_window[0]/2),int(size_window[1]/2)]\n",
    "        \n",
    "        try:\n",
    "            if ((shared_v[0] !=0) & (shared_v[1] !=0)):\n",
    "                pos_remote_head[0] = shared_v[0]\n",
    "                pos_remote_head[1] = shared_v[1]\n",
    "\n",
    "        except:\n",
    "            pos_remote_head = (int(size_window[0]/2),int(size_window[1]/2))\n",
    "            \n",
    "        R_w = (Rw_lt[0]+pos_remote_head[0], Rw_lt[1]+pos_remote_head[1])\n",
    "        Pw = (self.Ps[0]*(R_w[0]-Rs[0]/2)/Rs[0], self.Ps[1]*(R_w[1]-Rs[1]/2)/Rs[1], 0)\n",
    "\n",
    "        # get Pe\n",
    "        self.Pe[2] = -(self.f*conf.P_IDP)/np.sqrt((R_le[0]-R_re[0])**2 + (R_le[1]-R_re[1])**2)\n",
    "        # x-axis needs flip\n",
    "        self.Pe[0] = -np.abs(self.Pe[2])*(R_le[0]+R_re[0]-size_video[0])/(2*self.f) + self.Pc[0]\n",
    "        self.Pe[1] = np.abs(self.Pe[2])*(R_le[1]+R_re[1]-size_video[1])/(2*self.f) + self.Pc[1]\n",
    "\n",
    "        # calcualte alpha\n",
    "        a_w2z_x = math.degrees(math.atan( (Pw[0]-self.Pe[0])/(Pw[2]-self.Pe[2])))\n",
    "        a_w2z_y = math.degrees(math.atan( (Pw[1]-self.Pe[1])/(Pw[2]-self.Pe[2])))    \n",
    "\n",
    "        a_z2c_x = math.degrees(math.atan( (self.Pe[0]-self.Pc[0])/(self.Pc[2]-self.Pe[2])))\n",
    "        a_z2c_y = math.degrees(math.atan( (self.Pe[1]-self.Pc[1])/(self.Pc[2]-self.Pe[2])))\n",
    "\n",
    "        alpha = [int(a_w2z_y + a_z2c_y),int(a_w2z_x + a_z2c_x)] # (V,H)\n",
    "\n",
    "        return alpha, self.Pe, R_w\n",
    "    \n",
    "    def flx_gaze(self, frame, gray, detections, shared_v, lock, pixel_cut=[3,4], size_I = [48,64]):\n",
    "        alpha_w2c = [0,0]\n",
    "        x_ratio = size_video[0]/self.size_df[0]\n",
    "        y_ratio = size_video[1]/self.size_df[1]\n",
    "        LE_M_A=[]\n",
    "        RE_M_A=[]\n",
    "        p_e=[0,0]\n",
    "        R_w=[0,0]\n",
    "        for k,bx in enumerate(detections):\n",
    "            # Get facial landmarks\n",
    "            time_start = time.time()\n",
    "            target_bx = dlib.rectangle(left=int(bx.left()*x_ratio),right =int(bx.right()*x_ratio),\n",
    "                                       top =int(bx.top()*y_ratio), bottom=int(bx.bottom()*y_ratio))\n",
    "            shape = self.predictor(gray, target_bx)\n",
    "            # get eye\n",
    "            LE_img, LE_M_A, LE_center, size_le_ori, R_le_LT = self.get_inputs(frame, shape, pos=\"L\", size_I=size_I)\n",
    "            RE_img, RE_M_A, RE_center, size_re_ori, R_re_LT = self.get_inputs(frame, shape, pos=\"R\", size_I=size_I)\n",
    "            # shifting angles estimator\n",
    "            alpha_w2c, p_e, R_w = self.shifting_angles_estimator(LE_center,RE_center,shared_v,lock)\n",
    "            \n",
    "            time_get_eye = time.time() - time_start\n",
    "            # gaze manipulation\n",
    "            time_start = time.time()\n",
    "            \n",
    "            # gaze redirection\n",
    "            # left Eye\n",
    "            LE_infer_img = self.L_sess.run(self.LE_img_pred, feed_dict= {\n",
    "                                                            self.LE_input_img: np.expand_dims(LE_img, axis = 0),\n",
    "                                                            self.LE_input_fp: np.expand_dims(LE_M_A, axis = 0),\n",
    "                                                            self.LE_input_ang: np.expand_dims(alpha_w2c, axis = 0),\n",
    "                                                            self.LE_phase_train: False\n",
    "                                                         })\n",
    "            LE_infer = cv2.resize(LE_infer_img.reshape(size_I[0],size_I[1],3), (size_le_ori[1], size_le_ori[0]))\n",
    "            # right Eye\n",
    "            RE_infer_img = self.R_sess.run(self.RE_img_pred, feed_dict= {\n",
    "                                                            self.RE_input_img: np.expand_dims(RE_img, axis = 0),\n",
    "                                                            self.RE_input_fp: np.expand_dims(RE_M_A, axis = 0),\n",
    "                                                            self.RE_input_ang: np.expand_dims(alpha_w2c, axis = 0),\n",
    "                                                            self.RE_phase_train: False\n",
    "                                                         })\n",
    "            RE_infer = cv2.resize(RE_infer_img.reshape(size_I[0],size_I[1],3), (size_re_ori[1], size_re_ori[0]))\n",
    "            \n",
    "            # repace eyes\n",
    "            frame[(R_le_LT[0]+pixel_cut[0]):(R_le_LT[0]+size_le_ori[0]-pixel_cut[0]),\n",
    "                  (R_le_LT[1]+pixel_cut[1]):(R_le_LT[1]+size_le_ori[1]-pixel_cut[1])] = LE_infer[pixel_cut[0]:(-1*pixel_cut[0]), pixel_cut[1]:-1*(pixel_cut[1])]*255\n",
    "            frame[(R_re_LT[0]+pixel_cut[0]):(R_re_LT[0]+size_re_ori[0]-pixel_cut[0]),\n",
    "                  (R_re_LT[1]+pixel_cut[1]):(R_re_LT[1]+size_re_ori[1]-pixel_cut[1])] = RE_infer[pixel_cut[0]:(-1*pixel_cut[0]), pixel_cut[1]:-1*(pixel_cut[1])]*255\n",
    "\n",
    "        frame = self.monitor_para(frame, alpha_w2c, self.Pe, R_w)\n",
    "\n",
    "        result, imgencode = cv2.imencode('.jpg', frame, self.encode_param)\n",
    "        data = pickle.dumps(imgencode, 0)\n",
    "        self.client_socket.sendall(struct.pack(\"L\", len(data)) + data)\n",
    "        return True\n",
    "        \n",
    "    def redirect_gaze(self, frame,shared_v,lock):\n",
    "        # head detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        face_detect_gray = cv2.resize(gray,(self.size_df[0],self.size_df[1]))\n",
    "        detections = self.detector(face_detect_gray, 0)\n",
    "           \n",
    "        rg_thread = Thread(target=self.flx_gaze, args=(frame, gray, detections,shared_v,lock))\n",
    "        rg_thread.start()\n",
    "        return True\n",
    "    \n",
    "    def run(self,shared_v,lock):\n",
    "        # def main():\n",
    "        redir = False\n",
    "        size_window = [659,528]\n",
    "        vs = cv2.VideoCapture(0)\n",
    "        vs.set(3, size_video[0])\n",
    "        vs.set(4, size_video[1])\n",
    "        t = time.time()\n",
    "        cv2.namedWindow(conf.uid)\n",
    "        cv2.moveWindow(conf.uid, int(Rs[0]/2)-int(size_window[0]/2),int(Rs[1]/2)-int(size_window[1]/2));\n",
    "        while 1:\n",
    "            ret, recv_frame = vs.read()\n",
    "            if ret:\n",
    "                cv2.imshow(conf.uid,recv_frame)\n",
    "                if recv_frame is not None:\n",
    "                    # redirected gaze\n",
    "                    if redir:\n",
    "                        frame = recv_frame.copy()\n",
    "                        try:\n",
    "                            tag = self.redirect_gaze(frame,shared_v,lock)\n",
    "                        except:\n",
    "                            pass\n",
    "                    else:\n",
    "                        result, imgencode = cv2.imencode('.jpg', recv_frame, self.encode_param)\n",
    "                        data = pickle.dumps(imgencode, 0)\n",
    "                        self.client_socket.sendall(struct.pack(\"L\", len(data)) + data)\n",
    "\n",
    "                    if (time.time() - t) > 1:\n",
    "                        t = time.time()\n",
    "\n",
    "                    k = cv2.waitKey(10)\n",
    "                    if k == ord('q'):\n",
    "                        data = pickle.dumps('stop')\n",
    "                        self.client_socket.sendall(struct.pack(\"L\", len(data))+data)\n",
    "                        time.sleep(3)\n",
    "                        cv2.destroyWindow(conf.uid)\n",
    "                        self.client_socket.shutdown(socket.SHUT_RDWR)\n",
    "                        self.client_socket.close()\n",
    "                        vs.release()\n",
    "                        self.L_sess.close()\n",
    "                        self.R_sess.close()\n",
    "                        break\n",
    "                    elif k == ord('r'):\n",
    "                        if redir:\n",
    "                            redir = False\n",
    "                        else:\n",
    "                            redir = True\n",
    "                    else:\n",
    "                        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    l = mp.Lock()  # multi-process lock\n",
    "    v = mp.Array('i', [320,240])  # shared parameter\n",
    "    # start video receiver\n",
    "    # vs_thread = Thread(target=video_receiver, args=(conf.recver_port,))\n",
    "    vs_thread = mp.Process(target=video_receiver, args=(v,l))\n",
    "    vs_thread.start()\n",
    "    time.sleep(1)\n",
    "    gz_thread = mp.Process(target=gaze_redirection_system, args=(v,l))\n",
    "    gz_thread.start()\n",
    "    vs_thread.join()\n",
    "    gz_thread.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
